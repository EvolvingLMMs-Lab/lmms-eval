<p align="center" width="70%">
<img src="https://i.postimg.cc/KvkLzbF9/WX20241212-014400-2x.png">
</p>

# Evaluierungssuite f√ºr Gro√üe Multimodale Modelle

üåê [English](../../README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](README_zh-TW.md) | [Êó•Êú¨Ë™û](README_ja.md) | [ÌïúÍµ≠Ïñ¥](README_ko.md) | [Espa√±ol](README_es.md) | [Fran√ßais](README_fr.md) | **Deutsch** | [Portugu√™s](README_pt-BR.md) | [–†—É—Å—Å–∫–∏–π](README_ru.md) | [Italiano](README_it.md) | [Nederlands](README_nl.md) | [Polski](README_pl.md) | [T√ºrk√ße](README_tr.md) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README_ar.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](README_hi.md) | [Ti·∫øng Vi·ªát](README_vi.md) | [Indonesia](README_id.md)

[![PyPI](https://img.shields.io/pypi/v/lmms-eval)](https://pypi.org/project/lmms-eval)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmms-eval)
[![GitHub contributors](https://img.shields.io/github/contributors/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/graphs/contributors)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)
[![open issues](https://img.shields.io/github/issues-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)

> Beschleunigung der Entwicklung gro√üer multimodaler Modelle (LMMs) mit `lmms-eval`. Wir unterst√ºtzen die meisten Text-, Bild-, Video- und Audio-Aufgaben.

üè† [LMMs-Lab Homepage](https://www.lmms-lab.com/) | ü§ó [Huggingface Datens√§tze](https://huggingface.co/lmms-lab) | <a href="https://emoji.gg/emoji/1684-discord-thread"><img src="https://cdn3.emoji.gg/emojis/1684-discord-thread.png" width="14px" height="14px" alt="Discord_Thread"></a> [discord/lmms-eval](https://discord.gg/zdkwKUqrPy)

üìñ [Unterst√ºtzte Aufgaben (100+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/current_tasks.md) | üåü [Unterst√ºtzte Modelle (30+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main/lmms_eval/models) | üìö [Dokumentation](../README.md)

---

## Ank√ºndigungen

- [2025-10] üöÄüöÄ **LMMs-Eval v0.5** ist da! Diese Hauptversion f√ºhrt umfassende Audio-Evaluierung, Response-Caching, 5 neue Modelle (GPT-4o Audio Preview, Gemma-3, LongViLA-R1, LLaVA-OneVision 1.5, Thyme) und √ºber 50 neue Benchmark-Varianten ein, die Audio (Step2, VoiceBench, WenetSpeech), Vision (CharXiv, Lemonade) und Reasoning (CSBench, SciBench, MedQA, SuperGPQA) abdecken. Details finden Sie in den [Release Notes](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.5.md).
- [2025-07] üöÄüöÄ Wir haben `lmms-eval-0.4` ver√∂ffentlicht. Details finden Sie in den [Release Notes](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.4.md).

## Warum `lmms-eval`?

Wir befinden uns auf einer aufregenden Reise zur Schaffung K√ºnstlicher Allgemeiner Intelligenz (AGI), √§hnlich wie die Begeisterung der Mondlandung in den 1960er Jahren. Diese Reise wird von fortschrittlichen gro√üen Sprachmodellen (LLMs) und gro√üen multimodalen Modellen (LMMs) angetrieben, komplexen Systemen, die in der Lage sind, eine Vielzahl menschlicher Aufgaben zu verstehen, zu lernen und auszuf√ºhren.

Um zu messen, wie fortschrittlich diese Modelle sind, verwenden wir verschiedene Evaluierungs-Benchmarks. Diese Benchmarks sind Werkzeuge, die uns helfen, die F√§higkeiten dieser Modelle zu verstehen und zeigen, wie nah wir der Erreichung von AGI sind. Das Finden und Verwenden dieser Benchmarks ist jedoch eine gro√üe Herausforderung.

Im Bereich der Sprachmodelle hat die Arbeit von [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) einen wertvollen Pr√§zedenzfall geschaffen. Wir haben das exquisite und effiziente Design von lm-evaluation-harness aufgenommen und **lmms-eval** eingef√ºhrt, ein sorgf√§ltig entwickeltes Evaluierungs-Framework f√ºr konsistente und effiziente Evaluierung von LMM.

## Installation

### Verwendung von uv (Empfohlen f√ºr konsistente Umgebungen)

Wir verwenden `uv` f√ºr die Paketverwaltung, um sicherzustellen, dass alle Entwickler exakt dieselben Paketversionen verwenden. Installieren Sie zun√§chst uv:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

F√ºr die Entwicklung mit konsistenter Umgebung:
```bash
git clone https://github.com/EvolvingLMMs-Lab/lmms-eval
cd lmms-eval
# Empfohlen
uv pip install -e ".[all]"
# Wenn Sie uv sync verwenden m√∂chten
# uv sync  # Dies erstellt/aktualisiert Ihre Umgebung aus uv.lock
```

Um Befehle auszuf√ºhren:
```bash
uv run python -m lmms_eval --help  # Beliebigen Befehl mit uv run ausf√ºhren
```

### Alternative Installation

F√ºr direkte Verwendung von Git:
```bash
uv venv eval
uv venv --python 3.12
source eval/bin/activate
# M√∂glicherweise m√ºssen Sie Ihre eigene Task-YAML hinzuf√ºgen und einbinden, wenn Sie diese Installation verwenden
uv pip install git+https://github.com/EvolvingLMMs-Lab/lmms-eval.git
```

## Verwendung

> Weitere Beispiele in [examples/models](../../examples/models)

**Evaluierung eines OpenAI-kompatiblen Modells**

```bash
bash examples/models/openai_compatible.sh
bash examples/models/xai_grok.sh
```

**Evaluierung von vLLM**

```bash
bash examples/models/vllm_qwen2vl.sh
```

**Evaluierung von LLaVA-OneVision**

```bash
bash examples/models/llava_onevision.sh
```

**Weitere Parameter**

```bash
python3 -m lmms_eval --help
```

## Benutzerdefiniertes Modell und Datensatz Hinzuf√ºgen

Siehe unsere [Dokumentation](../README.md).

## Danksagungen

lmms_eval ist ein Fork von [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness). Wir empfehlen, die [Dokumentation von lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs) f√ºr relevante Informationen zu lesen.

## Zitierung

```shell
@misc{zhang2024lmmsevalrealitycheckevaluation,
      title={LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models}, 
      author={Kaichen Zhang and Bo Li and Peiyuan Zhang and Fanyi Pu and Joshua Adrian Cahyono and Kairui Hu and Shuai Liu and Yuanhan Zhang and Jingkang Yang and Chunyuan Li and Ziwei Liu},
      year={2024},
      eprint={2407.12772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12772}, 
}
```
