<p align="center" width="70%">
<img src="https://i.postimg.cc/KvkLzbF9/WX20241212-014400-2x.png">
</p>

# Evaluierungssuite f√ºr Gro√üe Multimodale Modelle

üåê [English](../../README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](README_zh-TW.md) | [Êó•Êú¨Ë™û](README_ja.md) | [ÌïúÍµ≠Ïñ¥](README_ko.md) | [Espa√±ol](README_es.md) | [Fran√ßais](README_fr.md) | **Deutsch** | [Portugu√™s](README_pt-BR.md) | [–†—É—Å—Å–∫–∏–π](README_ru.md) | [Italiano](README_it.md) | [Nederlands](README_nl.md) | [Polski](README_pl.md) | [T√ºrk√ße](README_tr.md) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README_ar.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](README_hi.md) | [Ti·∫øng Vi·ªát](README_vi.md) | [Indonesia](README_id.md)

[![PyPI](https://img.shields.io/pypi/v/lmms-eval)](https://pypi.org/project/lmms-eval)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmms-eval)
[![GitHub contributors](https://img.shields.io/github/contributors/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/graphs/contributors)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)
[![open issues](https://img.shields.io/github/issues-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)

> Beschleunigung der Entwicklung gro√üer multimodaler Modelle (LMMs) mit `lmms-eval`. Wir unterst√ºtzen die meisten Text-, Bild-, Video- und Audio-Aufgaben.

üè† [LMMs-Lab Homepage](https://www.lmms-lab.com/) | ü§ó [Huggingface Datens√§tze](https://huggingface.co/lmms-lab) | <a href="https://emoji.gg/emoji/1684-discord-thread"><img src="https://cdn3.emoji.gg/emojis/1684-discord-thread.png" width="14px" height="14px" alt="Discord_Thread"></a> [discord/lmms-eval](https://discord.gg/zdkwKUqrPy)

üìñ [Unterst√ºtzte Aufgaben (100+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/current_tasks.md) | üåü [Unterst√ºtzte Modelle (30+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main/lmms_eval/models) | üìö [Dokumentation](../README.md)

---

## Was ist neu?

Die Evaluierung multimodaler Modelle ist schwieriger, als es aussieht. Wir haben hunderte von Benchmarks, aber keinen Standardweg, um sie auszuf√ºhren. Die Ergebnisse variieren zwischen den Laboren. Vergleiche werden unzuverl√§ssig. Wir haben daran gearbeitet, dies zu beheben ‚Äì nicht durch heldenhaften Einsatz, sondern durch systematische Prozesse.

**Januar 2026** ‚Äì Wir haben erkannt, dass r√§umliches und kompositionelles Denken blinde Flecken in bestehenden Benchmarks blieben. Wir haben [CaptionQA](https://captionqa.github.io/), [SpatialTreeBench](https://github.com/THUNLP-MT/SpatialTreeBench), [SiteBench](https://sitebench.github.io/) und [ViewSpatial](https://github.com/ViewSpatial/ViewSpatial) hinzugef√ºgt. F√ºr Teams, die Remote-Evaluierungs-Pipelines betreiben, haben wir einen HTTP-Eval-Server eingef√ºhrt (#972). F√ºr diejenigen, die statistische Strenge ben√∂tigen, haben wir CLT und Clustered Standard Error Estimation hinzugef√ºgt (#989).

**Oktober 2025 (v0.5)** ‚Äì Audio war eine L√ºcke. Modelle konnten h√∂ren, aber wir hatten keinen konsistenten Weg, sie zu testen. Dieses Release f√ºgte eine umfassende Audio-Evaluierung, Response-Caching f√ºr Effizienz und √ºber 50 Benchmark-Varianten hinzu, die Audio, Vision und Reasoning abdecken. [Release Notes](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.5.md).

<details>
<summary>Nachfolgend finden Sie eine chronologische Liste der j√ºngsten Aufgaben, Modelle und Funktionen, die von unseren gro√üartigen Mitwirkenden hinzugef√ºgt wurden. </summary>

- [2025-01] üéìüéì Wir haben unseren neuen Benchmark ver√∂ffentlicht: [Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos](https://arxiv.org/abs/2501.13826). Weitere Details finden Sie auf der [Projektseite](https://videommmu.github.io/).
- [2024-12] üéâüéâ Wir haben gemeinsam mit dem [MME-Team](https://github.com/BradyFU/Video-MME) und dem [OpenCompass-Team](https://github.com/open-compass) den [MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs](https://arxiv.org/pdf/2411.15296) vorgestellt.
- [2024-11] üîàüîä `lmms-eval/v0.3.0` wurde aktualisiert, um Audio-Evaluierungen f√ºr Audio-Modelle wie Qwen2-Audio und Gemini-Audio √ºber Aufgaben wie AIR-Bench, Clotho-AQA, LibriSpeech und mehr hinweg zu unterst√ºtzen. Weitere Details finden Sie im [Blog](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.3.md)!
- [2024-10] üéâüéâ Wir begr√º√üen die neue Aufgabe [NaturalBench](https://huggingface.co/datasets/BaiqiL/NaturalBench), ein visionszentrierter VQA-Benchmark (NeurIPS'24), der Vision-Language-Modelle mit einfachen Fragen zu nat√ºrlichen Bildern herausfordert.
- [2024-10] üéâüéâ Wir begr√º√üen die neue Aufgabe [TemporalBench](https://huggingface.co/datasets/microsoft/TemporalBench) f√ºr feingliedriges tempor√§res Verst√§ndnis und Schlussfolgern f√ºr Videos, die eine riesige (>30%) L√ºcke zwischen Mensch und KI aufdeckt.

</details>

## Warum `lmms-eval`?

Wir befinden uns auf einer aufregenden Reise zur Schaffung K√ºnstlicher Allgemeiner Intelligenz (AGI), √§hnlich wie die Begeisterung der Mondlandung in den 1960er Jahren. Diese Reise wird von fortschrittlichen gro√üen Sprachmodellen (LLMs) und gro√üen multimodalen Modellen (LMMs) angetrieben, komplexen Systemen, die in der Lage sind, eine Vielzahl menschlicher Aufgaben zu verstehen, zu lernen und auszuf√ºhren.

Um zu messen, wie fortschrittlich diese Modelle sind, verwenden wir verschiedene Evaluierungs-Benchmarks. Diese Benchmarks sind Werkzeuge, die uns helfen, die F√§higkeiten dieser Modelle zu verstehen und zeigen, wie nah wir der Erreichung von AGI sind. Das Finden und Verwenden dieser Benchmarks ist jedoch eine gro√üe Herausforderung.

Im Bereich der Sprachmodelle hat die Arbeit von [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) einen wertvollen Pr√§zedenzfall geschaffen. Wir haben das exquisite und effiziente Design von lm-evaluation-harness aufgenommen und **lmms-eval** eingef√ºhrt, ein sorgf√§ltig entwickeltes Evaluierungs-Framework f√ºr konsistente und effiziente Evaluierung von LMM.

## Installation

### Verwendung von uv (Empfohlen f√ºr konsistente Umgebungen)

Wir verwenden `uv` f√ºr die Paketverwaltung, um sicherzustellen, dass alle Entwickler exakt dieselben Paketversionen verwenden. Installieren Sie zun√§chst uv:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

F√ºr die Entwicklung mit konsistenter Umgebung:
```bash
git clone https://github.com/EvolvingLMMs-Lab/lmms-eval
cd lmms-eval
# Empfohlen
uv pip install -e ".[all]"
# Wenn Sie uv sync verwenden m√∂chten
# uv sync  # Dies erstellt/aktualisiert Ihre Umgebung aus uv.lock
```

Um Befehle auszuf√ºhren:
```bash
uv run python -m lmms_eval --help  # Beliebigen Befehl mit uv run ausf√ºhren
```

### Alternative Installation

F√ºr direkte Verwendung von Git:
```bash
uv venv eval
uv venv --python 3.12
source eval/bin/activate
# M√∂glicherweise m√ºssen Sie Ihre eigene Task-YAML hinzuf√ºgen und einbinden, wenn Sie diese Installation verwenden
uv pip install git+https://github.com/EvolvingLMMs-Lab/lmms-eval.git
```

## Verwendung

> Weitere Beispiele in [examples/models](../../examples/models)

**Evaluierung eines OpenAI-kompatiblen Modells**

```bash
bash examples/models/openai_compatible.sh
bash examples/models/xai_grok.sh
```

**Evaluierung von vLLM**

```bash
bash examples/models/vllm_qwen2vl.sh
```

**Evaluierung von LLaVA-OneVision**

```bash
bash examples/models/llava_onevision.sh
```

**Evaluierung von LLaVA-OneVision1_5**

```bash
bash examples/models/llava_onevision1_5.sh
```

**Evaluierung von LLaMA-3.2-Vision**

```bash
bash examples/models/llama_vision.sh
```

**Evaluierung von Qwen2-VL**

```bash
bash examples/models/qwen2_vl.sh
bash examples/models/qwen2_5_vl.sh
```

**Evaluierung von LLaVA auf MME**

Wenn Sie LLaVA 1.5 testen m√∂chten, m√ºssen Sie deren Repository von [LLaVA](https://github.com/haotian-liu/LLaVA) klonen und

```bash
bash examples/models/llava_next.sh
```

**Evaluierung mit Tensor Parallel f√ºr gr√∂√üere Modelle (llava-next-72b)**

```bash
bash examples/models/tensor_parallel.sh
```

**Evaluierung mit SGLang f√ºr gr√∂√üere Modelle (llava-next-72b)**

```bash
bash examples/models/sglang.sh
```

**Evaluierung mit vLLM f√ºr gr√∂√üere Modelle (llava-next-72b)**

```bash
bash examples/models/vllm_qwen2vl.sh
```

**Weitere Parameter**

```bash
python3 -m lmms_eval --help
```

**Umgebungsvariablen**
Bevor Sie Experimente und Evaluierungen durchf√ºhren, empfehlen wir Ihnen, die folgenden Umgebungsvariablen in Ihre Umgebung zu exportieren. Einige sind f√ºr die Ausf√ºhrung bestimmter Aufgaben erforderlich.

```bash
export OPENAI_API_KEY="<YOUR_API_KEY>"
export HF_HOME="<Path to HF cache>" 
export HF_TOKEN="<YOUR_API_KEY>"
export HF_HUB_ENABLE_HF_TRANSFER="1"
export REKA_API_KEY="<YOUR_API_KEY>"
# Weitere m√∂gliche Umgebungsvariablen sind 
# ANTHROPIC_API_KEY, DASHSCOPE_API_KEY etc.
```

**H√§ufige Umgebungsprobleme**

Manchmal treten h√§ufige Probleme auf, zum Beispiel Fehler im Zusammenhang mit `httpx` oder `protobuf`. Um diese Probleme zu l√∂sen, k√∂nnen Sie zun√§chst versuchen:

```bash
python3 -m pip install httpx==0.23.3;
python3 -m pip install protobuf==3.20;
# Wenn Sie numpy==2.x verwenden, kann dies manchmal Fehler verursachen
python3 -m pip install numpy==1.26;
# Manchmal ist sentencepiece erforderlich, damit der Tokenizer funktioniert
python3 -m pip install sentencepiece;
```

## Benutzerdefiniertes Modell und Datensatz Hinzuf√ºgen

Siehe unsere [Dokumentation](../README.md).

## Danksagungen

lmms_eval ist ein Fork von [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness). Wir empfehlen, die [Dokumentation von lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs) f√ºr relevante Informationen zu lesen.

## Zitierung

```shell
@misc{zhang2024lmmsevalrealitycheckevaluation,
      title={LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models}, 
      author={Kaichen Zhang and Bo Li and Peiyuan Zhang and Fanyi Pu and Joshua Adrian Cahyono and Kairui Hu and Shuai Liu and Yuanhan Zhang and Jingkang Yang and Chunyuan Li and Ziwei Liu},
      year={2024},
      eprint={2407.12772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12772}, 
}

@misc{lmms_eval2024,
    title={LMMs-Eval: Accelerating the Development of Large Multimoal Models},
    url={https://github.com/EvolvingLMMs-Lab/lmms-eval},
    author={Bo Li*, Peiyuan Zhang*, Kaichen Zhang*, Fanyi Pu*, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li and Ziwei Liu},
    publisher    = {Zenodo},
    version      = {v0.1.0},
    month={March},
    year={2024}
}
```
