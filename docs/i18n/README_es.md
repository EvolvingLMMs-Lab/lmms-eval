<p align="center" width="70%">
<img src="https://i.postimg.cc/KvkLzbF9/WX20241212-014400-2x.png">
</p>

# Suite de Evaluaci√≥n de Modelos Multimodales de Gran Escala

üåê [English](../../README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](README_zh-TW.md) | [Êó•Êú¨Ë™û](README_ja.md) | [ÌïúÍµ≠Ïñ¥](README_ko.md) | **Espa√±ol** | [Fran√ßais](README_fr.md) | [Deutsch](README_de.md) | [Portugu√™s](README_pt-BR.md) | [–†—É—Å—Å–∫–∏–π](README_ru.md) | [Italiano](README_it.md) | [Nederlands](README_nl.md) | [Polski](README_pl.md) | [T√ºrk√ße](README_tr.md) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README_ar.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](README_hi.md) | [Ti·∫øng Vi·ªát](README_vi.md) | [Indonesia](README_id.md)

[![PyPI](https://img.shields.io/pypi/v/lmms-eval)](https://pypi.org/project/lmms-eval)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmms-eval)
[![GitHub contributors](https://img.shields.io/github/contributors/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/graphs/contributors)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)
[![open issues](https://img.shields.io/github/issues-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)

> Acelerando el desarrollo de modelos multimodales de gran escala (LMMs) con `lmms-eval`. Soportamos la mayor√≠a de tareas de texto, imagen, video y audio.

üè† [P√°gina Principal de LMMs-Lab](https://www.lmms-lab.com/) | ü§ó [Conjuntos de Datos de Huggingface](https://huggingface.co/lmms-lab) | <a href="https://emoji.gg/emoji/1684-discord-thread"><img src="https://cdn3.emoji.gg/emojis/1684-discord-thread.png" width="14px" height="14px" alt="Discord_Thread"></a> [discord/lmms-eval](https://discord.gg/zdkwKUqrPy)

üìñ [Tareas Soportadas (100+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/current_tasks.md) | üåü [Modelos Soportados (30+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main/lmms_eval/models) | üìö [Documentaci√≥n](../README.md)

---

## Anuncios

Evaluar modelos multimodales es m√°s dif√≠cil de lo que parece. Tenemos cientos de benchmarks, pero no hay una forma est√°ndar de ejecutarlos. Los resultados var√≠an entre laboratorios. Las comparaciones se vuelven poco fiables. Hemos estado trabajando para abordar esto, no mediante esfuerzos heroicos, sino mediante procesos sistem√°ticos.

**Enero de 2026** - Reconocimos que el razonamiento espacial y composicional segu√≠an siendo puntos ciegos en los benchmarks existentes. A√±adimos [CaptionQA](https://captionqa.github.io/), [SpatialTreeBench](https://github.com/THUNLP-MT/SpatialTreeBench), [SiteBench](https://sitebench.github.io/), y [ViewSpatial](https://github.com/ViewSpatial/ViewSpatial). Para los equipos que ejecutan flujos de evaluaci√≥n remotos, introdujimos un servidor de evaluaci√≥n HTTP (#972). Para quienes necesitan rigor estad√≠stico, a√±adimos CLT y estimaci√≥n de error est√°ndar por cl√∫ster (#989).

**Octubre de 2025 (v0.5)** - El audio hab√≠a sido una brecha. Los modelos pod√≠an o√≠r, pero no ten√≠amos una forma consistente de probarlos. Esta versi√≥n a√±adi√≥ una evaluaci√≥n de audio completa, cach√© de respuestas para mayor eficiencia y m√°s de 50 variantes de benchmarks que abarcan audio, visi√≥n y razonamiento. [Notas de la versi√≥n](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.5.md).

<details>
<summary>A continuaci√≥n se presenta una lista cronol√≥gica de las tareas, modelos y caracter√≠sticas recientes a√±adidos por nuestros incre√≠bles colaboradores.</summary>

- [2025-01] üéìüéì Hemos lanzado nuestro nuevo benchmark: [Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos](https://arxiv.org/abs/2501.13826). Consulte la [p√°gina del proyecto](https://videommmu.github.io/) para m√°s detalles.
- [2024-12] üéâüéâ Hemos presentado [MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs](https://arxiv.org/pdf/2411.15296), conjuntamente con el [Equipo MME](https://github.com/BradyFU/Video-MME) y el [Equipo OpenCompass](https://github.com/open-compass).
- [2024-11] üîàüîä El `lmms-eval/v0.3.0` ha sido actualizado para soportar evaluaciones de audio para modelos de audio como Qwen2-Audio y Gemini-Audio en tareas como AIR-Bench, Clotho-AQA, LibriSpeech, y m√°s. ¬°Consulte el [blog](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.3.md) para m√°s detalles!

</details>

## ¬øPor qu√© `lmms-eval`?

Estamos en un emocionante viaje hacia la creaci√≥n de Inteligencia General Artificial (AGI), similar al entusiasmo del aterrizaje lunar de los a√±os 60. Este viaje est√° impulsado por modelos de lenguaje de gran escala (LLMs) y modelos multimodales de gran escala (LMMs), sistemas complejos capaces de entender, aprender y realizar una amplia variedad de tareas humanas.

Para medir cu√°n avanzados son estos modelos, utilizamos una variedad de benchmarks de evaluaci√≥n. Estos benchmarks son herramientas que nos ayudan a entender las capacidades de estos modelos, mostr√°ndonos qu√© tan cerca estamos de lograr AGI. Sin embargo, encontrar y usar estos benchmarks es un gran desaf√≠o.

En el campo de los modelos de lenguaje, el trabajo de [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) ha establecido un precedente valioso. Absorbimos el dise√±o exquisito y eficiente de lm-evaluation-harness e introducimos **lmms-eval**, un framework de evaluaci√≥n meticulosamente elaborado para la evaluaci√≥n consistente y eficiente de LMM.

## Instalaci√≥n

### Usando uv (Recomendado para entornos consistentes)

Usamos `uv` para la gesti√≥n de paquetes para asegurar que todos los desarrolladores usen exactamente las mismas versiones de paquetes. Primero, instale uv:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Para desarrollo con entorno consistente:
```bash
git clone https://github.com/EvolvingLMMs-Lab/lmms-eval
cd lmms-eval
# Recomendado
uv pip install -e ".[all]"
# Si desea usar uv sync
# uv sync  # Esto crea/actualiza su entorno desde uv.lock
```

Para ejecutar comandos:
```bash
uv run python -m lmms_eval --help  # Ejecutar cualquier comando con uv run
```

### Instalaci√≥n Alternativa

Para uso directo desde Git:
```bash
uv venv eval
uv venv --python 3.12
source eval/bin/activate
# Puede que necesite agregar e incluir su propio yaml de tareas si usa esta instalaci√≥n
uv pip install git+https://github.com/EvolvingLMMs-Lab/lmms-eval.git
```

## Uso

> M√°s ejemplos en [examples/models](../../examples/models)

**Evaluaci√≥n de Modelo Compatible con OpenAI**

```bash
bash examples/models/openai_compatible.sh
bash examples/models/xai_grok.sh
```

**Evaluaci√≥n de vLLM**

```bash
bash examples/models/vllm_qwen2vl.sh
```

**Evaluaci√≥n de LLaVA-OneVision**

```bash
bash examples/models/llava_onevision.sh
```

**Evaluaci√≥n de LLaVA-OneVision1_5**

```bash
bash examples/models/llava_onevision1_5.sh
```

**Evaluaci√≥n de LLaMA-3.2-Vision**

```bash
bash examples/models/llama_vision.sh
```

**Evaluaci√≥n de Qwen2-VL**

```bash
bash examples/models/qwen2_vl.sh
bash examples/models/qwen2_5_vl.sh
```

**Evaluaci√≥n de LLaVA en MME**

Si desea probar LLaVA 1.5, tendr√° que clonar su repositorio de [LLaVA](https://github.com/haotian-liu/LLaVA) y

```bash
bash examples/models/llava_next.sh
```

**Evaluaci√≥n con tensor parallel para modelos m√°s grandes (llava-next-72b)**

```bash
bash examples/models/tensor_parallel.sh
```

**Evaluaci√≥n con SGLang para modelos m√°s grandes (llava-next-72b)**

```bash
bash examples/models/sglang.sh
```

**M√°s Par√°metros**

```bash
python3 -m lmms_eval --help
```

## Variables de Entorno
Antes de ejecutar experimentos y evaluaciones, le recomendamos exportar las siguientes variables de entorno a su entorno. Algunas son necesarias para que ciertas tareas funcionen.

```bash
export OPENAI_API_KEY="<YOUR_API_KEY>"
export HF_HOME="<Path to HF cache>" 
export HF_TOKEN="<YOUR_API_KEY>"
export HF_HUB_ENABLE_HF_TRANSFER="1"
export REKA_API_KEY="<YOUR_API_KEY>"
# Otras posibles variables de entorno incluyen 
# ANTHROPIC_API_KEY, DASHSCOPE_API_KEY etc.
```

## Problemas Comunes del Entorno

A veces puede encontrar algunos problemas comunes, por ejemplo, errores relacionados con httpx o protobuf. Para resolver estos problemas, primero puede intentar:

```bash
python3 -m pip install httpx==0.23.3;
python3 -m pip install protobuf==3.20;
# Si est√° usando numpy==2.x, a veces puede causar errores
python3 -m pip install numpy==1.26;
# A veces se requiere sentencepiece para que el tokenizador funcione
python3 -m pip install sentencepiece;
```

## Agregar Modelo y Conjunto de Datos Personalizados

Consulte nuestra [documentaci√≥n](../README.md).

## Reconocimientos

lmms_eval es un fork de [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness). Recomendamos leer la [documentaci√≥n de lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs) para informaci√≥n relevante.

## Citas

```shell
@misc{zhang2024lmmsevalrealitycheckevaluation,
      title={LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models}, 
      author={Kaichen Zhang and Bo Li and Peiyuan Zhang and Fanyi Pu and Joshua Adrian Cahyono and Kairui Hu and Shuai Liu and Yuanhan Zhang and Jingkang Yang and Chunyuan Li and Ziwei Liu},
      year={2024},
      eprint={2407.12772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12772}, 
}

@misc{lmms_eval2024,
    title={LMMs-Eval: Accelerating the Development of Large Multimoal Models},
    url={https://github.com/EvolvingLMMs-Lab/lmms-eval},
    author={Bo Li*, Peiyuan Zhang*, Kaichen Zhang*, Fanyi Pu*, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li and Ziwei Liu},
    publisher    = {Zenodo},
    version      = {v0.1.0},
    month={March},
    year={2024}
}
```
