<p align="center" width="70%">
<img src="https://i.postimg.cc/KvkLzbF9/WX20241212-014400-2x.png">
</p>

# Suite d'√âvaluation des Grands Mod√®les Multimodaux

üåê [English](../../README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](README_zh-TW.md) | [Êó•Êú¨Ë™û](README_ja.md) | [ÌïúÍµ≠Ïñ¥](README_ko.md) | [Espa√±ol](README_es.md) | **Fran√ßais** | [Deutsch](README_de.md) | [Portugu√™s](README_pt-BR.md) | [–†—É—Å—Å–∫–∏–π](README_ru.md) | [Italiano](README_it.md) | [Nederlands](README_nl.md) | [Polski](README_pl.md) | [T√ºrk√ße](README_tr.md) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README_ar.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](README_hi.md) | [Ti·∫øng Vi·ªát](README_vi.md) | [Indonesia](README_id.md)

[![PyPI](https://img.shields.io/pypi/v/lmms-eval)](https://pypi.org/project/lmms-eval)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmms-eval)
[![GitHub contributors](https://img.shields.io/github/contributors/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/graphs/contributors)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)
[![open issues](https://img.shields.io/github/issues-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)

> Acc√©l√©rer le d√©veloppement des grands mod√®les multimodaux (LMMs) avec `lmms-eval`. Nous supportons la plupart des t√¢ches de texte, d'image, de vid√©o et d'audio.

üè† [Page d'Accueil LMMs-Lab](https://www.lmms-lab.com/) | ü§ó [Jeux de Donn√©es Huggingface](https://huggingface.co/lmms-lab) | <a href="https://emoji.gg/emoji/1684-discord-thread"><img src="https://cdn3.emoji.gg/emojis/1684-discord-thread.png" width="14px" height="14px" alt="Discord_Thread"></a> [discord/lmms-eval](https://discord.gg/zdkwKUqrPy)

üìñ [T√¢ches Support√©es (100+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/current_tasks.md) | üåü [Mod√®les Support√©s (30+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main/lmms_eval/models) | üìö [Documentation](../README.md)

---

## Quoi de Neuf

√âvaluer des mod√®les multimodaux est plus difficile qu'il n'y para√Æt. Nous disposons de centaines de benchmarks, mais d'aucune m√©thode standard pour les ex√©cuter. Les r√©sultats varient d'un laboratoire √† l'autre. Les comparaisons deviennent peu fiables. Nous nous effor√ßons de rem√©dier √† ce probl√®me - non par un effort h√©ro√Øque, mais par un processus syst√©matique.

**Janvier 2026** - Nous avons reconnu que le raisonnement spatial et compositionnel restait un angle mort dans les benchmarks existants. Nous avons ajout√© [CaptionQA](https://captionqa.github.io/), [SpatialTreeBench](https://github.com/THUNLP-MT/SpatialTreeBench), [SiteBench](https://sitebench.github.io/), et [ViewSpatial](https://github.com/ViewSpatial/ViewSpatial). Pour les √©quipes g√©rant des pipelines d'√©valuation √† distance, nous avons introduit un serveur d'√©valuation HTTP (#972). Pour ceux qui ont besoin de rigueur statistique, nous avons ajout√© le CLT (th√©or√®me central limite) et l'estimation de l'erreur standard group√©e (#989).

**Octobre 2025 (v0.5)** - L'audio √©tait une lacune. Les mod√®les pouvaient entendre, mais nous n'avions aucun moyen coh√©rent de les tester. Cette version a ajout√© une √©valuation audio compl√®te, la mise en cache des r√©ponses pour plus d'efficacit√©, et plus de 50 variantes de benchmarks couvrant l'audio, la vision et le raisonnement. [Notes de version](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.5.md).

<details>
<summary>Ci-dessous une liste chronologique des t√¢ches, mod√®les et fonctionnalit√©s r√©cents ajout√©s par nos incroyables contributeurs. </summary>

- [2025-01] üéìüéì Nous avons publi√© notre nouveau benchmark : [Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos](https://arxiv.org/abs/2501.13826). Veuillez vous r√©f√©rer √† la [page du projet](https://videommmu.github.io/) pour plus de d√©tails.
- [2024-12] üéâüéâ Nous avons pr√©sent√© [MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs](https://arxiv.org/pdf/2411.15296), conjointement avec l'[√âquipe MME](https://github.com/BradyFU/Video-MME) et l'[√âquipe OpenCompass](https://github.com/open-compass).
- [2024-11] üîàüîä `lmms-eval/v0.3.0` a √©t√© mis √† jour pour supporter les √©valuations audio pour des mod√®les audio comme Qwen2-Audio et Gemini-Audio sur des t√¢ches telles que AIR-Bench, Clotho-AQA, LibriSpeech, et plus encore. Veuillez vous r√©f√©rer au [blog](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.3.md) pour plus de d√©tails !
- [2024-10] üéâüéâ Nous accueillons la nouvelle t√¢che [NaturalBench](https://huggingface.co/datasets/BaiqiL/NaturalBench), un benchmark VQA centr√© sur la vision (NeurIPS'24) qui d√©fie les mod√®les vision-langage avec des questions simples sur l'imagerie naturelle.
- [2024-10] üéâüéâ Nous accueillons la nouvelle t√¢che [TemporalBench](https://huggingface.co/datasets/microsoft/TemporalBench) pour une compr√©hension temporelle fine et un raisonnement pour les vid√©os, qui r√©v√®le un √©cart √©norme (>30%) entre l'humain et l'IA.
- [2024-10] üéâüéâ Nous accueillons les nouvelles t√¢ches [VDC](https://rese1f.github.io/aurora-web/) pour le l√©gendage d√©taill√© de vid√©os, [MovieChat-1K](https://rese1f.github.io/MovieChat/) pour la compr√©hension de vid√©os longue dur√©e, et [Vinoground](https://vinoground.github.io/), un benchmark LMM temporel contrefactuel compos√© de 1000 paires courtes de vid√©os-l√©gendes naturelles. Nous accueillons √©galement les nouveaux mod√®les : [AuroraCap](https://github.com/rese1f/aurora) et [MovieChat](https://github.com/rese1f/MovieChat).
- [2024-09] üéâüéâ Nous accueillons les nouvelles t√¢ches [MMSearch](https://mmsearch.github.io/) et [MME-RealWorld](https://mme-realworld.github.io/) pour l'acc√©l√©ration de l'inf√©rence.
- [2024-09] ‚öôÔ∏èÔ∏è‚öôÔ∏èÔ∏èÔ∏èÔ∏è Nous mettons √† jour `lmms-eval` vers `0.2.3` avec plus de t√¢ches et de fonctionnalit√©s. Nous supportons un ensemble compact d'√©valuations de t√¢ches de langage (cr√©dit code √† [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)), et nous supprimons la logique d'enregistrement au d√©marrage (pour tous les mod√®les et t√¢ches) pour r√©duire la surcharge. D√©sormais, `lmms-eval` ne lance que les t√¢ches/mod√®les n√©cessaires. Veuillez consulter les [notes de version](https://github.com/EvolvingLMMs-Lab/lmms-eval/releases/tag/v0.2.3) pour plus de d√©tails.
- [2024-08] üéâüéâ Nous accueillons le nouveau mod√®le [LLaVA-OneVision](https://huggingface.co/papers/2408.03326), [Mantis](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/162), les nouvelles t√¢ches [MVBench](https://huggingface.co/datasets/OpenGVLab/MVBench), [LongVideoBench](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/117), [MMStar](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/158). Nous fournissons une nouvelle fonctionnalit√© d'API SGlang Runtime pour le mod√®le llava-onevision, veuillez vous r√©f√©rer au [doc](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/commands.md) pour l'acc√©l√©ration de l'inf√©rence.
- [2024-07] üë®‚Äçüíªüë®‚Äçüíª `lmms-eval/v0.2.1` a √©t√© mis √† jour pour supporter plus de mod√®les, incluant [LongVA](https://github.com/EvolvingLMMs-Lab/LongVA), [InternVL-2](https://github.com/OpenGVLab/InternVL), [VILA](https://github.com/NVlabs/VILA), et bien d'autres t√¢ches d'√©valuation, par exemple [Details Captions](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/136), [MLVU](https://arxiv.org/abs/2406.04264), [WildVision-Bench](https://huggingface.co/datasets/WildVision/wildvision-arena-data), [VITATECS](https://github.com/lscpku/VITATECS) et [LLaVA-Interleave-Bench](https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/).
- [2024-07] üéâüéâ Nous avons publi√© le [rapport technique](https://arxiv.org/abs/2407.12772) et [LiveBench](https://huggingface.co/spaces/lmms-lab/LiveBench) !
- [2024-06] üé¨üé¨ `lmms-eval/v0.2.0` a √©t√© mis √† jour pour supporter les √©valuations vid√©o pour des mod√®les vid√©o comme LLaVA-NeXT Video et Gemini 1.5 Pro sur des t√¢ches telles que EgoSchema, PerceptionTest, VideoMME, et plus encore. Veuillez vous r√©f√©rer au [blog](https://lmms-lab.github.io/posts/lmms-eval-0.2/) pour plus de d√©tails !
- [2024-03] üìùüìù Nous avons publi√© la premi√®re version de `lmms-eval`, veuillez vous r√©f√©rer au [blog](https://lmms-lab.github.io/posts/lmms-eval-0.1/) pour plus de d√©tails !

</details>

## Pourquoi `lmms-eval` ?

Nous sommes dans un voyage passionnant vers la cr√©ation de l'Intelligence Artificielle G√©n√©rale (AGI), similaire √† l'enthousiasme de l'alunissage des ann√©es 1960. Ce voyage est propuls√© par des mod√®les de langage avanc√©s (LLMs) et des grands mod√®les multimodaux (LMMs), des syst√®mes complexes capables de comprendre, d'apprendre et d'effectuer une grande vari√©t√© de t√¢ches humaines.

Pour mesurer l'avancement de ces mod√®les, nous utilisons une vari√©t√© de benchmarks d'√©valuation. Ces benchmarks sont des outils qui nous aident √† comprendre les capacit√©s de ces mod√®les, nous montrant √† quel point nous sommes proches d'atteindre l'AGI. Cependant, trouver et utiliser ces benchmarks est un d√©fi majeur.

Dans le domaine des mod√®les de langage, le travail de [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) a √©tabli un pr√©c√©dent pr√©cieux. Nous avons absorb√© la conception exquise et efficace de lm-evaluation-harness et introduit **lmms-eval**, un framework d'√©valuation m√©ticuleusement con√ßu pour une √©valuation coh√©rente et efficace des LMM.

## Installation

### Utilisation de uv (Recommand√© pour des environnements coh√©rents)

Nous utilisons `uv` pour la gestion des paquets afin de garantir que tous les d√©veloppeurs utilisent exactement les m√™mes versions de paquets. Tout d'abord, installez uv :
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Pour le d√©veloppement avec un environnement coh√©rent :
```bash
git clone https://github.com/EvolvingLMMs-Lab/lmms-eval
cd lmms-eval
# Recommand√©
uv pip install -e ".[all]"
# Si vous voulez utiliser uv sync
# uv sync  # Ceci cr√©e/met √† jour votre environnement depuis uv.lock
```

Pour ex√©cuter des commandes :
```bash
uv run python -m lmms_eval --help  # Ex√©cuter n'importe quelle commande avec uv run
```

### Installation Alternative

Pour une utilisation directe depuis Git :
```bash
uv venv eval
uv venv --python 3.12
source eval/bin/activate
# Vous devrez peut-√™tre ajouter et inclure votre propre yaml de t√¢ches si vous utilisez cette installation
uv pip install git+https://github.com/EvolvingLMMs-Lab/lmms-eval.git
```

## Utilisation

> Plus d'exemples dans [examples/models](../../examples/models)

**√âvaluation de Mod√®le Compatible OpenAI**

```bash
bash examples/models/openai_compatible.sh
bash examples/models/xai_grok.sh
```

**√âvaluation de vLLM**

```bash
bash examples/models/vllm_qwen2vl.sh
```

**√âvaluation de LLaVA-OneVision**

```bash
bash examples/models/llava_onevision.sh
```

**√âvaluation de LLaVA-OneVision1_5**

```bash
bash examples/models/llava_onevision1_5.sh
```

**√âvaluation de LLaMA-3.2-Vision**

```bash
bash examples/models/llama_vision.sh
```

**√âvaluation de Qwen2-VL**

```bash
bash examples/models/qwen2_vl.sh
bash examples/models/qwen2_5_vl.sh
```

**√âvaluation de LLaVA sur MME**

Si vous voulez tester LLaVA 1.5, vous devrez cloner leur d√©p√¥t depuis [LLaVA](https://github.com/haotian-liu/LLaVA) et

```bash
bash examples/models/llava_next.sh
```

**√âvaluation avec parall√©lisme de tenseurs pour les mod√®les plus grands (llava-next-72b)**

```bash
bash examples/models/tensor_parallel.sh
```

**√âvaluation avec SGLang pour les mod√®les plus grands (llava-next-72b)**

```bash
bash examples/models/sglang.sh
```

**√âvaluation avec vLLM pour les mod√®les plus grands (llava-next-72b)**

```bash
bash examples/models/vllm_qwen2vl.sh
```

**Plus de Param√®tres**

```bash
python3 -m lmms_eval --help
```

**Variables d'Environnement**
Avant d'ex√©cuter des exp√©riences et des √©valuations, nous vous recommandons d'exporter les variables d'environnement suivantes dans votre environnement. Certaines sont n√©cessaires pour l'ex√©cution de certaines t√¢ches.

```bash
export OPENAI_API_KEY="<YOUR_API_KEY>"
export HF_HOME="<Path to HF cache>" 
export HF_TOKEN="<YOUR_API_KEY>"
export HF_HUB_ENABLE_HF_TRANSFER="1"
export REKA_API_KEY="<YOUR_API_KEY>"
# Other possible environment variables include 
# ANTHROPIC_API_KEY,DASHSCOPE_API_KEY etc.
```

**Probl√®mes d'Environnement Courants**

Parfois, vous pourriez rencontrer des probl√®mes courants, par exemple des erreurs li√©es √† httpx ou protobuf. Pour r√©soudre ces probl√®mes, vous pouvez d'abord essayer :

```bash
python3 -m pip install httpx==0.23.3;
python3 -m pip install protobuf==3.20;
# If you are using numpy==2.x, sometimes may causing errors
python3 -m pip install numpy==1.26;
# Someties sentencepiece are required for tokenizer to work
python3 -m pip install sentencepiece;
```

## Ajouter un Mod√®le et un Jeu de Donn√©es Personnalis√©s

Consultez notre [documentation](../README.md).

## Remerciements

lmms_eval est un fork de [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness). Nous vous recommandons de lire la [documentation de lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs) pour des informations pertinentes.

## Citations

```shell
@misc{zhang2024lmmsevalrealitycheckevaluation,
      title={LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models}, 
      author={Kaichen Zhang and Bo Li and Peiyuan Zhang and Fanyi Pu and Joshua Adrian Cahyono and Kairui Hu and Shuai Liu and Yuanhan Zhang and Jingkang Yang and Chunyuan Li and Ziwei Liu},
      year={2024},
      eprint={2407.12772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12772}, 
}

@misc{lmms_eval2024,
    title={LMMs-Eval: Accelerating the Development of Large Multimoal Models},
    url={https://github.com/EvolvingLMMs-Lab/lmms-eval},
    author={Bo Li*, Peiyuan Zhang*, Kaichen Zhang*, Fanyi Pu*, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li and Ziwei Liu},
    publisher    = {Zenodo},
    version      = {v0.1.0},
    month={March},
    year={2024}
}
``````
