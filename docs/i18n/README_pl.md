<p align="center" width="70%">
<img src="https://i.postimg.cc/KvkLzbF9/WX20241212-014400-2x.png">
</p>

# Pakiet Ewaluacyjny dla Du≈ºych Modeli Multimodalnych

üåê [English](../../README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](README_zh-TW.md) | [Êó•Êú¨Ë™û](README_ja.md) | [ÌïúÍµ≠Ïñ¥](README_ko.md) | [Espa√±ol](README_es.md) | [Fran√ßais](README_fr.md) | [Deutsch](README_de.md) | [Portugu√™s](README_pt-BR.md) | [–†—É—Å—Å–∫–∏–π](README_ru.md) | [Italiano](README_it.md) | [Nederlands](README_nl.md) | **Polski** | [T√ºrk√ße](README_tr.md) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README_ar.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](README_hi.md) | [Ti·∫øng Vi·ªát](README_vi.md) | [Indonesia](README_id.md)

[![PyPI](https://img.shields.io/pypi/v/lmms-eval)](https://pypi.org/project/lmms-eval)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmms-eval)
[![GitHub contributors](https://img.shields.io/github/contributors/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/graphs/contributors)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)
[![open issues](https://img.shields.io/github/issues-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)

> Przyspieszenie rozwoju du≈ºych modeli multimodalnych (LMMs) z `lmms-eval`. Obs≈Çugujemy wiƒôkszo≈õƒá zada≈Ñ tekstowych, obrazowych, wideo i audio.

üè† [Strona G≈Ç√≥wna LMMs-Lab](https://www.lmms-lab.com/) | ü§ó [Zbiory Danych Huggingface](https://huggingface.co/lmms-lab) | <a href="https://emoji.gg/emoji/1684-discord-thread"><img src="https://cdn3.emoji.gg/emojis/1684-discord-thread.png" width="14px" height="14px" alt="Discord_Thread"></a> [discord/lmms-eval](https://discord.gg/zdkwKUqrPy)

üìñ [Obs≈Çugiwane Zadania (100+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/current_tasks.md) | üåü [Obs≈Çugiwane Modele (30+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main/lmms_eval/models) | üìö [Dokumentacja](../README.md)

---

## Og≈Çoszenia

- [2025-10] üöÄüöÄ **LMMs-Eval v0.5** jest tutaj! Ta g≈Ç√≥wna wersja wprowadza kompleksowƒÖ ewaluacjƒô audio, buforowanie odpowiedzi, 5 nowych modeli (GPT-4o Audio Preview, Gemma-3, LongViLA-R1, LLaVA-OneVision 1.5, Thyme) oraz ponad 50 nowych wariant√≥w benchmark√≥w obejmujƒÖcych audio (Step2, VoiceBench, WenetSpeech), wizjƒô (CharXiv, Lemonade) i rozumowanie (CSBench, SciBench, MedQA, SuperGPQA). Szczeg√≥≈Çy w [notatkach wydania](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.5.md).
- [2025-07] üöÄüöÄ Wydali≈õmy `lmms-eval-0.4`. Szczeg√≥≈Çy w [notatkach wydania](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.4.md).

## Dlaczego `lmms-eval`?

Jeste≈õmy w ekscytujƒÖcej podr√≥≈ºy ku stworzeniu Sztucznej Og√≥lnej Inteligencji (AGI), podobnej do entuzjazmu lƒÖdowania na Ksiƒô≈ºycu w latach 60. Ta podr√≥≈º jest napƒôdzana przez zaawansowane du≈ºe modele jƒôzykowe (LLMs) i du≈ºe modele multimodalne (LMMs), z≈Ço≈ºone systemy zdolne do rozumienia, uczenia siƒô i wykonywania szerokiej gamy ludzkich zada≈Ñ.

Aby zmierzyƒá, jak zaawansowane sƒÖ te modele, u≈ºywamy r√≥≈ºnych benchmark√≥w ewaluacyjnych. Te benchmarki sƒÖ narzƒôdziami, kt√≥re pomagajƒÖ nam zrozumieƒá mo≈ºliwo≈õci tych modeli, pokazujƒÖc, jak blisko jeste≈õmy osiƒÖgniƒôcia AGI. Jednak znalezienie i wykorzystanie tych benchmark√≥w jest du≈ºym wyzwaniem.

W dziedzinie modeli jƒôzykowych praca [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) ustanowi≈Ça cenny precedens. Przyswoili≈õmy wyrafinowany i efektywny design lm-evaluation-harness i wprowadzili≈õmy **lmms-eval**, starannie opracowany framework ewaluacyjny do sp√≥jnej i efektywnej ewaluacji LMM.

## Instalacja

### U≈ºywajƒÖc uv (Zalecane dla sp√≥jnych ≈õrodowisk)

U≈ºywamy `uv` do zarzƒÖdzania pakietami, aby zapewniƒá, ≈ºe wszyscy programi≈õci u≈ºywajƒÖ dok≈Çadnie tych samych wersji pakiet√≥w. Najpierw zainstaluj uv:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Do rozwoju ze sp√≥jnym ≈õrodowiskiem:
```bash
git clone https://github.com/EvolvingLMMs-Lab/lmms-eval
cd lmms-eval
# Zalecane
uv pip install -e ".[all]"
# Je≈õli chcesz u≈ºywaƒá uv sync
# uv sync  # To tworzy/aktualizuje twoje ≈õrodowisko z uv.lock
```

Aby uruchamiaƒá polecenia:
```bash
uv run python -m lmms_eval --help  # Uruchom dowolne polecenie z uv run
```

### Alternatywna Instalacja

Do bezpo≈õredniego u≈ºycia z Git:
```bash
uv venv eval
uv venv --python 3.12
source eval/bin/activate
# Mo≈ºesz potrzebowaƒá dodaƒá i do≈ÇƒÖczyƒá w≈Çasny yaml zada≈Ñ, je≈õli u≈ºywasz tej instalacji
uv pip install git+https://github.com/EvolvingLMMs-Lab/lmms-eval.git
```

## U≈ºycie

> Wiƒôcej przyk≈Çad√≥w w [examples/models](../../examples/models)

**Ewaluacja Modelu Kompatybilnego z OpenAI**

```bash
bash examples/models/openai_compatible.sh
bash examples/models/xai_grok.sh
```

**Ewaluacja vLLM**

```bash
bash examples/models/vllm_qwen2vl.sh
```

**Ewaluacja LLaVA-OneVision**

```bash
bash examples/models/llava_onevision.sh
```

**Wiƒôcej Parametr√≥w**

```bash
python3 -m lmms_eval --help
```

## Dodawanie Niestandardowego Modelu i Zbioru Danych

Zobacz naszƒÖ [dokumentacjƒô](../README.md).

## Podziƒôkowania

lmms_eval jest forkiem [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness). Zalecamy przeczytanie [dokumentacji lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs) w celu uzyskania istotnych informacji.

## Cytowania

```shell
@misc{zhang2024lmmsevalrealitycheckevaluation,
      title={LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models}, 
      author={Kaichen Zhang and Bo Li and Peiyuan Zhang and Fanyi Pu and Joshua Adrian Cahyono and Kairui Hu and Shuai Liu and Yuanhan Zhang and Jingkang Yang and Chunyuan Li and Ziwei Liu},
      year={2024},
      eprint={2407.12772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12772}, 
}
```
