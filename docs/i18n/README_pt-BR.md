<p align="center" width="70%">
<img src="https://i.postimg.cc/KvkLzbF9/WX20241212-014400-2x.png">
</p>

# Suite de Avalia√ß√£o de Grandes Modelos Multimodais

üåê [English](../../README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](README_zh-TW.md) | [Êó•Êú¨Ë™û](README_ja.md) | [ÌïúÍµ≠Ïñ¥](README_ko.md) | [Espa√±ol](README_es.md) | [Fran√ßais](README_fr.md) | [Deutsch](README_de.md) | **Portugu√™s** | [–†—É—Å—Å–∫–∏–π](README_ru.md) | [Italiano](README_it.md) | [Nederlands](README_nl.md) | [Polski](README_pl.md) | [T√ºrk√ße](README_tr.md) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README_ar.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](README_hi.md) | [Ti·∫øng Vi·ªát](README_vi.md) | [Indonesia](README_id.md)

[![PyPI](https://img.shields.io/pypi/v/lmms-eval)](https://pypi.org/project/lmms-eval)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmms-eval)
[![GitHub contributors](https://img.shields.io/github/contributors/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/graphs/contributors)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)
[![open issues](https://img.shields.io/github/issues-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)

> Acelerando o desenvolvimento de grandes modelos multimodais (LMMs) com `lmms-eval`. Suportamos a maioria das tarefas de texto, imagem, v√≠deo e √°udio.

üè† [P√°gina Inicial LMMs-Lab](https://www.lmms-lab.com/) | ü§ó [Conjuntos de Dados Huggingface](https://huggingface.co/lmms-lab) | <a href="https://emoji.gg/emoji/1684-discord-thread"><img src="https://cdn3.emoji.gg/emojis/1684-discord-thread.png" width="14px" height="14px" alt="Discord_Thread"></a> [discord/lmms-eval](https://discord.gg/zdkwKUqrPy)

üìñ [Tarefas Suportadas (100+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/current_tasks.md) | üåü [Modelos Suportados (30+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main/lmms_eval/models) | üìö [Documenta√ß√£o](../README.md)

---

## O que h√° de novo

Avaliar modelos multimodais √© mais dif√≠cil do que parece. Temos centenas de benchmarks, mas nenhuma forma padronizada de execut√°-los. Os resultados variam entre laborat√≥rios. As compara√ß√µes tornam-se n√£o confi√°veis. Temos trabalhado para resolver isso - n√£o atrav√©s de um esfor√ßo her√≥ico, mas atrav√©s de um processo sistem√°tico.

**Janeiro de 2026** - Reconhecemos que o racioc√≠nio espacial e composicional permaneciam pontos cegos nos benchmarks existentes. Adicionamos [CaptionQA](https://captionqa.github.io/), [SpatialTreeBench](https://github.com/THUNLP-MT/SpatialTreeBench), [SiteBench](https://sitebench.github.io/) e [ViewSpatial](https://github.com/ViewSpatial/ViewSpatial). Para equipes que executam pipelines de avalia√ß√£o remota, introduzimos um servidor de avalia√ß√£o HTTP (#972). Para aqueles que precisam de rigor estat√≠stico, adicionamos CLT e estimativa de erro padr√£o agrupado (#989).

**Outubro de 2025 (v0.5)** - O √°udio era uma lacuna. Os modelos podiam ouvir, mas n√£o t√≠nhamos uma forma consistente de test√°-los. Este lan√ßamento adicionou avalia√ß√£o de √°udio abrangente, cache de respostas para efici√™ncia e mais de 50 variantes de benchmarks abrangendo √°udio, vis√£o e racioc√≠nio. [Notas de lan√ßamento](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.5.md).

<details>
<summary>Abaixo est√° uma lista cronol√≥gica de tarefas, modelos e recursos recentes adicionados pelos nossos incr√≠veis colaboradores. </summary>

- [2025-01] üéìüéì Lan√ßamos nosso novo benchmark: [Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos](https://arxiv.org/abs/2501.13826). Consulte a [p√°gina do projeto](https://videommmu.github.io/) para mais detalhes.
- [2024-12] üéâüéâ Apresentamos o [MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs](https://arxiv.org/pdf/2411.15296), juntamente com a [Equipe MME](https://github.com/BradyFU/Video-MME) e a [Equipe OpenCompass](https://github.com/open-compass).
- [2024-11] üîàüîä O `lmms-eval/v0.3.0` foi atualizado para suportar avalia√ß√µes de √°udio para modelos de √°udio como Qwen2-Audio e Gemini-Audio em tarefas como AIR-Bench, Clotho-AQA, LibriSpeech e muito mais. Consulte o [blog](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.3.md) para mais detalhes!
- [2024-10] üéâüéâ Damos as boas-vindas √† nova tarefa [NaturalBench](https://huggingface.co/datasets/BaiqiL/NaturalBench), um benchmark VQA focado em vis√£o (NeurIPS'24) que desafia modelos de vis√£o e linguagem com perguntas simples sobre imagens naturais.
- [2024-10] üéâüéâ Damos as boas-vindas √† nova tarefa [TemporalBench](https://huggingface.co/datasets/microsoft/TemporalBench) para compreens√£o temporal detalhada e racioc√≠nio para v√≠deos, que revela uma enorme lacuna de mais de 30% entre humanos e IA.
- [2024-10] üéâüéâ Damos as boas-vindas √†s novas tarefas [VDC](https://rese1f.github.io/aurora-web/) para legendagem detalhada de v√≠deo, [MovieChat-1K](https://rese1f.github.io/MovieChat/) para compreens√£o de v√≠deo de formato longo e [Vinoground](https://vinoground.github.io/), um benchmark LMM temporal contrafactual composto por 1000 pares curtos de v√≠deo-legenda naturais. Tamb√©m damos as boas-vindas aos novos modelos: [AuroraCap](https://github.com/rese1f/aurora) e [MovieChat](https://github.com/rese1f/MovieChat).
- [2024-09] üéâüéâ Damos as boas-vindas √†s novas tarefas [MMSearch](https://mmsearch.github.io/) e [MME-RealWorld](https://mme-realworld.github.io/) para acelera√ß√£o de infer√™ncia.
- [2024-09] ‚öôÔ∏èÔ∏è‚öôÔ∏èÔ∏èÔ∏èÔ∏è Atualizamos o `lmms-eval` para `0.2.3` com mais tarefas e recursos. Suportamos um conjunto compacto de avalia√ß√µes de tarefas de linguagem (cr√©dito de c√≥digo para [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)), e removemos a l√≥gica de registro no in√≠cio (para todos os modelos e tarefas) para reduzir a sobrecarga. Agora o `lmms-eval` lan√ßa apenas as tarefas/modelos necess√°rios. Verifique as [notas de lan√ßamento](https://github.com/EvolvingLMMs-Lab/lmms-eval/releases/tag/v0.2.3) para mais detalhes.
- [2024-08] üéâüéâ Damos as boas-vindas ao novo modelo [LLaVA-OneVision](https://huggingface.co/papers/2408.03326), [Mantis](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/162), novas tarefas [MVBench](https://huggingface.co/datasets/OpenGVLab/MVBench), [LongVideoBench](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/117), [MMStar](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/158). Fornecemos o novo recurso de SGlang Runtime API para o modelo llava-onevision, consulte o [documento](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/commands.md) para acelera√ß√£o de infer√™ncia.
- [2024-07] üë®‚Äçüíªüë®‚Äçüíª O `lmms-eval/v0.2.1` foi atualizado para suportar mais modelos, incluindo [LongVA](https://github.com/EvolvingLMMs-Lab/LongVA), [InternVL-2](https://github.com/OpenGVLab/InternVL), [VILA](https://github.com/NVlabs/VILA) e muitas outras tarefas de avalia√ß√£o, por exemplo, [Details Captions](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/136), [MLVU](https://arxiv.org/abs/2406.04264), [WildVision-Bench](https://huggingface.co/datasets/WildVision/wildvision-arena-data), [VITATECS](https://github.com/lscpku/VITATECS) e [LLaVA-Interleave-Bench](https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/).
- [2024-07] üéâüéâ Lan√ßamos o [relat√≥rio t√©cnico](https://arxiv.org/abs/2407.12772) e o [LiveBench](https://huggingface.co/spaces/lmms-lab/LiveBench)! 
- [2024-06] üé¨üé¨ O `lmms-eval/v0.2.0` foi atualizado para suportar avalia√ß√µes de v√≠deo para modelos de v√≠deo como LLaVA-NeXT Video e Gemini 1.5 Pro em tarefas como EgoSchema, PerceptionTest, VideoMME e muito mais. Consulte o [blog](https://lmms-lab.github.io/posts/lmms-eval-0.2/) para mais detalhes!
- [2024-03] üìùüìù Lan√ßamos a primeira vers√£o do `lmms-eval`, consulte o [blog](https://lmms-lab.github.io/posts/lmms-eval-0.1/) para mais detalhes!

</details>

## Por que `lmms-eval`?

Estamos em uma jornada emocionante em dire√ß√£o √† cria√ß√£o da Intelig√™ncia Artificial Geral (AGI), semelhante ao entusiasmo da alunissagem dos anos 1960. Esta jornada √© impulsionada por modelos de linguagem avan√ßados (LLMs) e grandes modelos multimodais (LMMs), sistemas complexos capazes de entender, aprender e executar uma ampla variedade de tarefas humanas.

Para medir o qu√£o avan√ßados esses modelos s√£o, usamos uma variedade de benchmarks de avalia√ß√£o. Esses benchmarks s√£o ferramentas que nos ajudam a entender as capacidades desses modelos, mostrando-nos o qu√£o perto estamos de alcan√ßar AGI. No entanto, encontrar e usar esses benchmarks √© um grande desafio.

No campo dos modelos de linguagem, o trabalho de [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) estabeleceu um precedente valioso. Absorvemos o design requintado e eficiente do lm-evaluation-harness e introduzimos o **lmms-eval**, um framework de avalia√ß√£o meticulosamente elaborado para avalia√ß√£o consistente e eficiente de LMM.

## Instala√ß√£o

### Usando uv (Recomendado para ambientes consistentes)

Usamos `uv` para gerenciamento de pacotes para garantir que todos os desenvolvedores usem exatamente as mesmas vers√µes de pacotes. Primeiro, instale o uv:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Para desenvolvimento com ambiente consistente:
```bash
git clone https://github.com/EvolvingLMMs-Lab/lmms-eval
cd lmms-eval
# Recomendado
uv pip install -e ".[all]"
# Se voc√™ quiser usar uv sync
# uv sync  # Isso cria/atualiza seu ambiente a partir de uv.lock
```

Para executar comandos:
```bash
uv run python -m lmms_eval --help  # Executar qualquer comando com uv run
```

### Instala√ß√£o Alternativa

Para uso direto do Git:
```bash
uv venv eval
uv venv --python 3.12
source eval/bin/activate
# Voc√™ pode precisar adicionar e incluir seu pr√≥prio yaml de tarefas se usar esta instala√ß√£o
uv pip install git+https://github.com/EvolvingLMMs-Lab/lmms-eval.git
```

## Uso

> Mais exemplos em [examples/models](../../examples/models)

**Avalia√ß√£o de Modelo Compat√≠vel com OpenAI**

```bash
bash examples/models/openai_compatible.sh
bash examples/models/xai_grok.sh
```

**Avalia√ß√£o de vLLM**

```bash
bash examples/models/vllm_qwen2vl.sh
```

**Avalia√ß√£o de LLaVA-OneVision**

```bash
bash examples/models/llava_onevision.sh
```

**Avalia√ß√£o de LLaVA-OneVision1_5**

```bash
bash examples/models/llava_onevision1_5.sh
```

**Avalia√ß√£o de LLaMA-3.2-Vision**

```bash
bash examples/models/llama_vision.sh
```

**Avalia√ß√£o de Qwen2-VL**

```bash
bash examples/models/qwen2_vl.sh
bash examples/models/qwen2_5_vl.sh
```

**Avalia√ß√£o de LLaVA no MME**

Se voc√™ quiser testar o LLaVA 1.5, voc√™ ter√° que clonar o reposit√≥rio deles de [LLaVA](https://github.com/haotian-liu/LLaVA) e

```bash
bash examples/models/llava_next.sh
```

**Avalia√ß√£o com paralelismo de tensores para modelos maiores (llava-next-72b)**

```bash
bash examples/models/tensor_parallel.sh
```

**Avalia√ß√£o com SGLang para modelos maiores (llava-next-72b)**

```bash
bash examples/models/sglang.sh
```

**Avalia√ß√£o com vLLM para modelos maiores (llava-next-72b)**

```bash
bash examples/models/vllm_qwen2vl.sh
```

**Mais Par√¢metros**

```bash
python3 -m lmms_eval --help
```

**Vari√°veis de Ambiente**
Antes de executar experimentos e avalia√ß√µes, recomendamos exportar as seguintes vari√°veis de ambiente. Algumas s√£o necess√°rias para a execu√ß√£o de certas tarefas.

```bash
export OPENAI_API_KEY="<YOUR_API_KEY>"
export HF_HOME="<Path to HF cache>" 
export HF_TOKEN="<YOUR_API_KEY>"
export HF_HUB_ENABLE_HF_TRANSFER="1"
export REKA_API_KEY="<YOUR_API_KEY>"
# Outras poss√≠veis vari√°veis de ambiente incluem 
# ANTHROPIC_API_KEY, DASHSCOPE_API_KEY etc.
```

**Problemas Comuns de Ambiente**

√Äs vezes, voc√™ pode encontrar problemas comuns, por exemplo, erros relacionados ao httpx ou protobuf. Para resolver esses problemas, voc√™ pode tentar primeiro:

```bash
python3 -m pip install httpx==0.23.3;
python3 -m pip install protobuf==3.20;
# Se voc√™ estiver usando numpy==2.x, √†s vezes pode causar erros
python3 -m pip install numpy==1.26;
# √Äs vezes, sentencepiece √© necess√°rio para o tokenizer funcionar
python3 -m pip install sentencepiece;
```

## Adicionar Modelo e Conjunto de Dados Personalizados

Consulte nossa [documenta√ß√£o](../README.md).

## Agradecimentos

lmms_eval √© um fork de [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness). Recomendamos ler a [documenta√ß√£o do lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs) para informa√ß√µes relevantes.

## Cita√ß√µes

```shell
@misc{zhang2024lmmsevalrealitycheckevaluation,
      title={LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models}, 
      author={Kaichen Zhang and Bo Li and Peiyuan Zhang and Fanyi Pu and Joshua Adrian Cahyono and Kairui Hu and Shuai Liu and Yuanhan Zhang and Jingkang Yang and Chunyuan Li and Ziwei Liu},
      year={2024},
      eprint={2407.12772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12772}, 
}

@misc{lmms_eval2024,
    title={LMMs-Eval: Accelerating the Development of Large Multimoal Models},
    url={https://github.com/EvolvingLMMs-Lab/lmms-eval},
    author={Bo Li*, Peiyuan Zhang*, Kaichen Zhang*, Fanyi Pu*, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li and Ziwei Liu},
    publisher    = {Zenodo},
    version      = {v0.1.0},
    month={March},
    year={2024}
}
```
