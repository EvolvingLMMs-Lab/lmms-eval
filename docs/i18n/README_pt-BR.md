<p align="center" width="70%">
<img src="https://i.postimg.cc/KvkLzbF9/WX20241212-014400-2x.png">
</p>

# Suite de Avalia√ß√£o de Grandes Modelos Multimodais

üåê [English](../../README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](README_zh-TW.md) | [Êó•Êú¨Ë™û](README_ja.md) | [ÌïúÍµ≠Ïñ¥](README_ko.md) | [Espa√±ol](README_es.md) | [Fran√ßais](README_fr.md) | [Deutsch](README_de.md) | **Portugu√™s** | [–†—É—Å—Å–∫–∏–π](README_ru.md) | [Italiano](README_it.md) | [Nederlands](README_nl.md) | [Polski](README_pl.md) | [T√ºrk√ße](README_tr.md) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README_ar.md) | [‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](README_hi.md) | [Ti·∫øng Vi·ªát](README_vi.md) | [Indonesia](README_id.md)

[![PyPI](https://img.shields.io/pypi/v/lmms-eval)](https://pypi.org/project/lmms-eval)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmms-eval)
[![GitHub contributors](https://img.shields.io/github/contributors/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/graphs/contributors)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)
[![open issues](https://img.shields.io/github/issues-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)

> Acelerando o desenvolvimento de grandes modelos multimodais (LMMs) com `lmms-eval`. Suportamos a maioria das tarefas de texto, imagem, v√≠deo e √°udio.

üè† [P√°gina Inicial LMMs-Lab](https://www.lmms-lab.com/) | ü§ó [Conjuntos de Dados Huggingface](https://huggingface.co/lmms-lab) | <a href="https://emoji.gg/emoji/1684-discord-thread"><img src="https://cdn3.emoji.gg/emojis/1684-discord-thread.png" width="14px" height="14px" alt="Discord_Thread"></a> [discord/lmms-eval](https://discord.gg/zdkwKUqrPy)

üìñ [Tarefas Suportadas (100+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/current_tasks.md) | üåü [Modelos Suportados (30+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main/lmms_eval/models) | üìö [Documenta√ß√£o](../README.md)

---

## An√∫ncios

- [2025-10] üöÄüöÄ **LMMs-Eval v0.5** est√° aqui! Esta vers√£o principal introduz avalia√ß√£o de √°udio abrangente, cache de respostas, 5 novos modelos (GPT-4o Audio Preview, Gemma-3, LongViLA-R1, LLaVA-OneVision 1.5, Thyme), e mais de 50 novas variantes de benchmark abrangendo √°udio (Step2, VoiceBench, WenetSpeech), vis√£o (CharXiv, Lemonade) e racioc√≠nio (CSBench, SciBench, MedQA, SuperGPQA). Consulte as [notas de lan√ßamento](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.5.md) para detalhes.
- [2025-07] üöÄüöÄ Lan√ßamos `lmms-eval-0.4`. Consulte as [notas de lan√ßamento](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.4.md) para mais detalhes.

## Por que `lmms-eval`?

Estamos em uma jornada emocionante em dire√ß√£o √† cria√ß√£o da Intelig√™ncia Artificial Geral (AGI), semelhante ao entusiasmo da alunissagem dos anos 1960. Esta jornada √© impulsionada por modelos de linguagem avan√ßados (LLMs) e grandes modelos multimodais (LMMs), sistemas complexos capazes de entender, aprender e executar uma ampla variedade de tarefas humanas.

Para medir o qu√£o avan√ßados esses modelos s√£o, usamos uma variedade de benchmarks de avalia√ß√£o. Esses benchmarks s√£o ferramentas que nos ajudam a entender as capacidades desses modelos, mostrando-nos o qu√£o perto estamos de alcan√ßar AGI. No entanto, encontrar e usar esses benchmarks √© um grande desafio.

No campo dos modelos de linguagem, o trabalho de [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) estabeleceu um precedente valioso. Absorvemos o design requintado e eficiente do lm-evaluation-harness e introduzimos o **lmms-eval**, um framework de avalia√ß√£o meticulosamente elaborado para avalia√ß√£o consistente e eficiente de LMM.

## Instala√ß√£o

### Usando uv (Recomendado para ambientes consistentes)

Usamos `uv` para gerenciamento de pacotes para garantir que todos os desenvolvedores usem exatamente as mesmas vers√µes de pacotes. Primeiro, instale o uv:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Para desenvolvimento com ambiente consistente:
```bash
git clone https://github.com/EvolvingLMMs-Lab/lmms-eval
cd lmms-eval
# Recomendado
uv pip install -e ".[all]"
# Se voc√™ quiser usar uv sync
# uv sync  # Isso cria/atualiza seu ambiente a partir de uv.lock
```

Para executar comandos:
```bash
uv run python -m lmms_eval --help  # Executar qualquer comando com uv run
```

### Instala√ß√£o Alternativa

Para uso direto do Git:
```bash
uv venv eval
uv venv --python 3.12
source eval/bin/activate
# Voc√™ pode precisar adicionar e incluir seu pr√≥prio yaml de tarefas se usar esta instala√ß√£o
uv pip install git+https://github.com/EvolvingLMMs-Lab/lmms-eval.git
```

## Uso

> Mais exemplos em [examples/models](../../examples/models)

**Avalia√ß√£o de Modelo Compat√≠vel com OpenAI**

```bash
bash examples/models/openai_compatible.sh
bash examples/models/xai_grok.sh
```

**Avalia√ß√£o de vLLM**

```bash
bash examples/models/vllm_qwen2vl.sh
```

**Avalia√ß√£o de LLaVA-OneVision**

```bash
bash examples/models/llava_onevision.sh
```

**Mais Par√¢metros**

```bash
python3 -m lmms_eval --help
```

## Adicionar Modelo e Conjunto de Dados Personalizados

Consulte nossa [documenta√ß√£o](../README.md).

## Agradecimentos

lmms_eval √© um fork de [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness). Recomendamos ler a [documenta√ß√£o do lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs) para informa√ß√µes relevantes.

## Cita√ß√µes

```shell
@misc{zhang2024lmmsevalrealitycheckevaluation,
      title={LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models}, 
      author={Kaichen Zhang and Bo Li and Peiyuan Zhang and Fanyi Pu and Joshua Adrian Cahyono and Kairui Hu and Shuai Liu and Yuanhan Zhang and Jingkang Yang and Chunyuan Li and Ziwei Liu},
      year={2024},
      eprint={2407.12772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12772}, 
}
```
