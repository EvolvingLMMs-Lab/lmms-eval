# ImgEdit Benchmark Configuration
# Paper: ImgEdit: A Unified Image Editing Benchmark
# https://github.com/sysuyy/ImgEdit
#
# Dataset preparation:
#   1. Download Benchmark.tar from https://huggingface.co/datasets/sysuyy/ImgEdit/blob/main/Benchmark.tar
#   2. Extract: tar -xvf Benchmark.tar
#   3. Run prepare_dataset.py to convert JSON to HuggingFace dataset format:
#      python lmms_eval/tasks/imgedit/prepare_dataset.py \
#          --json_file /path/to/singleturn.json \
#          --img_root /path/to/singleturn/ \
#          --output_dir /path/to/output/dataset
#
# Environment variables:
#   - IMGEDIT_MODEL_NAME: Name of the model being evaluated (default: "default")
#   - IMGEDIT_OUTPUT_DIR: Directory to save generated images (default: "./logs/imgedit_results")
#   - IMGEDIT_ORIGIN_IMG_ROOT: Root directory of original images (for runtime loading)
#   - IMGEDIT_EVAL_BACKBONE: Evaluation model - "gpt4o" (default) or "qwen25vl"
#   - OPENAI_API_KEY: OpenAI API key (for GPT-4o)
#   - OPENAI_BASE_URL: Optional custom OpenAI API base URL
#   - QWEN_MODEL_PATH: Path to Qwen2.5-VL model (for qwen25vl backend)

# Local dataset path (after running prepare_dataset.py)
dataset_path: sysuyy/ImgEdit
dataset_kwargs:
  load_from_disk: True
task: "imgedit"
test_split: train  # The prepared dataset uses "train" split
output_type: generate_until

# Document processing functions
doc_to_visual: !function utils.imgedit_doc_to_visual
doc_to_text: !function utils.imgedit_doc_to_text
doc_to_target: "prompt"

# Generation parameters
generation_kwargs:
  max_new_tokens: 512
  temperature: 0
  top_p: 1.0
  num_beams: 1
  do_sample: false

# Process results using GPT-4o or Qwen2.5-VL evaluation (controlled by IMGEDIT_EVAL_BACKBONE)
process_results: !function utils.imgedit_process_results

# Metrics
# ==========================================
# Overall metrics (aggregated across all edit types)
# ==========================================
# Score1: Prompt Compliance / Style Fidelity / etc. (depends on edit type)
# Score2: Visual Naturalness / Content Preservation / etc.
# Score3: Physical & Detail Integrity / Rendering Quality / etc.
# avg_score: Average of Score1, Score2, Score3
metric_list:
  # Overall scores
  - metric: imgedit_score1
    aggregation: !function utils.imgedit_aggregate_results
    higher_is_better: true
  - metric: imgedit_score2
    aggregation: !function utils.imgedit_aggregate_results
    higher_is_better: true
  - metric: imgedit_score3
    aggregation: !function utils.imgedit_aggregate_results
    higher_is_better: true
  - metric: imgedit_avg_score
    aggregation: !function utils.imgedit_aggregate_results
    higher_is_better: true

  # ==========================================
  # Per-type average scores
  # ==========================================
  # Replace type
  - metric: imgedit_avg_score
    aggregation: !function utils.imgedit_aggregate_replace
    higher_is_better: true
    metric_name: imgedit_replace_score

  # Add type
  - metric: imgedit_avg_score
    aggregation: !function utils.imgedit_aggregate_add
    higher_is_better: true
    metric_name: imgedit_add_score

  # Adjust type
  - metric: imgedit_avg_score
    aggregation: !function utils.imgedit_aggregate_adjust
    higher_is_better: true
    metric_name: imgedit_adjust_score

  # Remove type
  - metric: imgedit_avg_score
    aggregation: !function utils.imgedit_aggregate_remove
    higher_is_better: true
    metric_name: imgedit_remove_score

  # Style type
  - metric: imgedit_avg_score
    aggregation: !function utils.imgedit_aggregate_style
    higher_is_better: true
    metric_name: imgedit_style_score

  # Action type
  - metric: imgedit_avg_score
    aggregation: !function utils.imgedit_aggregate_action
    higher_is_better: true
    metric_name: imgedit_action_score

  # Extract type
  - metric: imgedit_avg_score
    aggregation: !function utils.imgedit_aggregate_extract
    higher_is_better: true
    metric_name: imgedit_extract_score

  # Background type
  - metric: imgedit_avg_score
    aggregation: !function utils.imgedit_aggregate_background
    higher_is_better: true
    metric_name: imgedit_background_score

  # Compose type
  - metric: imgedit_avg_score
    aggregation: !function utils.imgedit_aggregate_compose
    higher_is_better: true
    metric_name: imgedit_compose_score

lmms_eval_specific_kwargs:
  default:
    pre_prompt: ""
    post_prompt: ""

metadata:
  - version: 0.2
    description: "ImgEdit Benchmark - Unified Image Editing Evaluation using GPT-4o or Qwen2.5-VL"
