dataset_path: parquet

task: "mmsi_attribute_meas_visual_cot"
dataset_kwargs:
  data_files: /blob/lmms-eval-dataset/mmsi_bench_5tasks/attribute_meas.parquet
test_split: train
output_type: generate_until
doc_to_visual: !function utils.msr_doc_to_visual
doc_to_text: !function utils.msr_doc_to_text_with_gen_prompt
doc_to_target: "answer"
process_results: !function utils.msr_process_results

lmms_eval_specific_kwargs:
  default:
    # Stage 1: Image generation prompt
    generation_prompt: "Create a visualization that highlights and annotates the measurable attributes (size, length, width, height, distance, area, volume) in the scene. Draw measurement lines, labels, and comparison markers to make quantitative relationships clear."
    # Stage 2: Question answering prompt
    pre_prompt: "You are given the original image(s) and a visualization highlighting measurable attributes. Use both to analyze size, length, width, height, distance, area, and volume relationships.\n\n"
    post_prompt: "\n\nBased on your measurement analysis, answer with the option's letter from the given choices directly. Enclose the option's letter within ``."

generation_kwargs:
  max_new_tokens: 2048
  temperature: 0
  do_sample: False

metric_list:
  - metric: Attribute (Meas.)
    aggregation: !function utils.msr_aggregate_results
    higher_is_better: true

metadata:
  - version: 0.0
  - description: "MMSI-Bench Attribute (Measurable) task with visual CoT two-stage inference"
