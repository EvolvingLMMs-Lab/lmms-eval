plm_stc:
  repo_id: facebook/PLM-VideoBench
  repo_type: dataset
  metadata_path: metadata/metadata_test.jsonl
  
  # The path to the SA-V (SAM-2) videos. Please download the dataset from https://ai.meta.com/datasets/segment-anything-video-downloads
  # Please ONLY DOWNLOAD THE TAR FILE NAMED "videos_fps_6.tar", which contains all the required videos.
  # Expected directory structure is
  # > video_base_dir
  #   > sav_001*.mp4
  #   > sav_002*.mp4
  #   > ...
  video_base_dir: <path/to/the/downloaded 6fps videos.>
  num_video_frames: 32

plm_fgqa:
  repo_id: facebook/PLM-VideoBench
  repo_type: dataset
  
  # The path to the PLM-FGQA videos.
  # Expected directory structure is
  # > video_base_dir
  #     > *.mp4
  video_base_dir: <path/to/the/downloaded FGQA videos.>
  num_video_frames: 32

plm_sgqa:
  repo_id: facebook/PLM-VideoBench
  repo_type: dataset
  
  # The path to the PLM-SGQA videos.
  # Expected directory structure is
  # > video_base_dir
  #     > *.mp4
  video_base_dir: <path/to/the/downloaded SGQA videos.>
  num_video_frames: 32

# We implement llm-judge using the OpenAI Python api.
# We use LLaMA-3.3-70B-Instruct as LLM judge.
# Many cloud providers, including LLaMA-API, Together.ai, and Groqcloud support the OpenAI api format.
# Please update the base_url, api_key, and model below to use the best cloud provider suitable for you.
# However, we recommend using LLaMA-API (https://llama.developer.meta.com) as all of our code has been tested with it.
llm_judge:
  base_url: <API base url>
  api_key: <API Key>
  model: Llama-3.3-70B-Instruct
