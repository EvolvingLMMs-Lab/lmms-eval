# StructEditBench (Structured-Visuals) task config for lmms-eval
#
# Expected dataset fields (minimum):
#   - source_image (PIL.Image) or input_image/image: source image for editing
#   - instruction/edit_prompt/prompt/edit_instruction: edit instruction for the model
#   - qa_list: list[{question, ground_truth_answer|answer, label(editing|maintain)}]
#   - category: one of {chart, math, graph, puzzle, science, table}
#
# Scoring backend (ImgEdit-style, OpenAI-compatible):
#   - Required env vars:
#       STRUCTEDITBENCH_API_KEY, STRUCTEDITBENCH_BASE_URL, STRUCTEDITBENCH_EVAL_MODEL_NAME
#   - Optional:
#       STRUCTEDITBENCH_JUDGE_MODEL_NAME, STRUCTEDITBENCH_TIMEOUT, STRUCTEDITBENCH_MAX_RETRIES, STRUCTEDITBENCH_CALL_DELAY, ...

dataset_path: parquet
dataset_kwargs:
  data_files:
    train: /path/to/StructEditBench/data/train-*.parquet

task: "structeditbench"
test_split: train
output_type: generate_until

doc_to_visual: !function utils.structeditbench_doc_to_visual
doc_to_text: !function utils.structeditbench_doc_to_text
doc_to_target: !function utils.structeditbench_doc_to_target

generation_kwargs:
  max_new_tokens: 512
  temperature: 0
  top_p: 1.0
  num_beams: 1
  do_sample: false

process_results: !function utils.structeditbench_process_results

metric_list:
  # Global metrics (percentage, higher is better)
  - metric: structeditbench_weighted_accuracy
    aggregation: !function utils.structeditbench_aggregate_score
    higher_is_better: true
  - metric: structeditbench_editing_accuracy
    aggregation: !function utils.structeditbench_aggregate_score
    higher_is_better: true
  - metric: structeditbench_maintain_accuracy
    aggregation: !function utils.structeditbench_aggregate_score
    higher_is_better: true

  # Category breakdown for weighted accuracy
  - metric: structeditbench_chart_weighted_accuracy
    aggregation: !function utils.structeditbench_aggregate_chart
    higher_is_better: true
  - metric: structeditbench_math_weighted_accuracy
    aggregation: !function utils.structeditbench_aggregate_math
    higher_is_better: true
  - metric: structeditbench_graph_weighted_accuracy
    aggregation: !function utils.structeditbench_aggregate_graph
    higher_is_better: true
  - metric: structeditbench_puzzle_weighted_accuracy
    aggregation: !function utils.structeditbench_aggregate_puzzle
    higher_is_better: true
  - metric: structeditbench_science_weighted_accuracy
    aggregation: !function utils.structeditbench_aggregate_science
    higher_is_better: true
  - metric: structeditbench_table_weighted_accuracy
    aggregation: !function utils.structeditbench_aggregate_table
    higher_is_better: true

lmms_eval_specific_kwargs:
  default:
    pre_prompt: ""
    post_prompt: ""

metadata:
  - version: 0.1
    description: "StructEditBench (Structured-Visuals) editing benchmark, scored via QA+judge (OpenAI-compatible VLM)"


